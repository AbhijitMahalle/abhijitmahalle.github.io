<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Abhijit Mahalle</title>
  
  <meta name="author" content="Abhijit Mahalle">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Abhijit Mahalle</name>
              </p>
              <p>
                I am a Robotics graduate from the University of Maryland, College Park. 
              </p>
              <p>
                Currently, I am looking for full-time role in Computer Vision, Robotics Software Development, or Machine learning.
              </p>
              <p>
                I have worked as a Computer Vision Research Assistant in the <a href="https://prg.cs.umd.edu/" target="_blank">Perception and Robotics Group</a> of UMD's 
                Computer Science Department.
              </p>
              <p style="text-align:center">
                <a target="_blank" href="mailto:amahalle60@gmail.com">Email</a> &nbsp;/&nbsp;
                <a target="_blank" href="data/AbhijitMahalle_Resume.pdf">Resume</a> &nbsp;/&nbsp;
                <a target="_blank" href="https://www.linkedin.com/in/abhijitmahalle">LinkedIn</a> &nbsp;/&nbsp;
                <a target="_blank" href="https://github.com/abhijitmahalle">GitHub</a> &nbsp;/&nbsp;
                <a target="_blank" href="https://leetcode.com/abhijit_mahalle">LeetCode</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/AbhijitMahalle.jpeg" target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="images/AbhijitMahalle_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:200px;width:100%;vertical-align:middle">
            <heading>EXPERIENCE</heading>           
          </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>       
          <tr>
            <td style="padding:20px;width:50%;vertical-align:middle">
              <a href="https://prg.cs.umd.edu/" target="_blank"><h3>Perception and Robotics Group, UMD</h3></a>
              <p>
                <b>
                  Research Assistant             Jun 2022 - May 2023
                </b>
              </p>  
              <p>
                <li>
                  Achieved an IoU score of 0.75 for segmentation and Average Endpoint Error of 0.15 m/s for motion estimation task by
                  training two encoder‚Äêdecoder networks to estimate scene depth, camera and object poses from the event camera output.
                </li>
              </p>
              <p>
                <li>
                  Combined the depth and pose estimates to generate optical flow to jointly optimize the two networks by unsupervised
                  learning.
                </li> 
              </p>
              <p>
                <li>
                  Assisted in creating a real‚Äêworld indoor ground‚Äêtruth dataset by fusing data‚Äêstreams from 3 event cameras, classical camera, and 2 IMUs to train VIO and SLAM deep learning models.
                </li> 
              </p>
              <p>
                <li>
                  Used Vicon motion capture system to get ground‚Äêtruth object poses and Mujuco simulator to generate ground‚Äêtruth depth and segmentation masks.
                </li> 
              </p>
              <p>
                <li>
                  Constructed a pipeline to calibrate event and classical cameras simultaneously by reconstructing grayscale images from the event‚Äêstream using deep neural network.
                </li> 
              </p>
            </td>
          </tr>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>       
            <tr>
              <td style="padding:20px;width:50%;vertical-align:middle">
                <a href="https://www.jacobs.com" target="_blank"><h3>Jacobs Engineering</h3></a>
                <p>
                  <b>
                    Design Engineer (CAD) &nbsp;&nbsp;&nbsp;&nbsp;            Sep 2018 - Jul 2021
                  </b>
                </p>  
                <p>
                  <li>
                    Prepared plot plan and equipment layouts as per statutory safety requirements, operability, and maintenance.
                  </li>
                </p>
                <p>
                  <li>
                    Designed piping systems using SP3D and PDMS 3D CAD software tools by considering pipe flexibility, pipe‚Äêsupport location, and allowable limits of 
                    equipment nozzle loads.
                  </li> 
                </p>
                <p>
                  <li>
                    Performed simulations using CAESAR II to analyze stresses on pipes due to temperature, pressure, self‚Äêweight, flow, and
                    mechanical vibrations, and selected appropriate location and type of pipe‚Äêsupport.                  
                  </li> 
                </p>
                <p>
                  <li>
                    Collaborated with 34 team members to understand client requirements and successfully deliver projects on time.
                  </li> 
                </p>
              </td>
            </tr>

          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Computer Vision / Machine Learning Projects</heading>           
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>       
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/sfm.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/structure-from-motion" target="_blank"><h3>Structure from Motion</h3></a>
              <p>
                Reconstructed a 3D scene and simultaneously obtained camera poses from a set of images using
                their feature point correspondence, epipolar geometry, triangulation, Persepective-n-Point (Pnp), bundle-adjustment, linear and non-linear optimization.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/pano1001.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/panorama-stitching" target="_blank"><h3>Panorama Stitching</h3></a>
              <p>
                Stitched multiple images to create a panorama using Harris Corner detection with Adaptive Non-Maximal Suppression for feature matching, RANSAC for removing 
                outliers, homography, and Poisson‚Äôs blending.
              </p>
              <p>
                Devised a CNN with supervised and unsupervised learning approach to estimate homography by generating a custom dataset and using 
                photometric loss and achieved a MSE of 55.59.
              </p>
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/faceswap.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/face-swap" target="_blank"><h3>Face Swap</h3></a>
              <p>
                Implemented a face-swap filter using dlib library from OpenCV to find facial landmarks. Used both Delaunay Triangulation with barycentric coordinates and 
                Thin Plate Spline to warp one face to another and compared the methods. 
              </p>
              <p>
                Leveraged Position Map Regression Network for face-swap using model-based approach.
              </p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/PbLite_3.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/probability-based-boundary-detection" target="_blank"><h3>PbLite Edge Detection</h3></a>
              <p>
                Detected edges using a simplified version of the probability of boundary detection algorithm by implementing custom filter-banks
                and using texton, brightness, and color maps.
              </p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/calib1.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/zhang-camera-calibration" target="_blank"><h3>Zhang's Camera Calibration</h3></a>
              <p>
                Implemented Zhang‚Äôs camera calibration technique with non-linear optimization to find the intrinsic and extrinsic parameters of a camera.
              </p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/visual-odometry.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/visual-odometry" target="_blank"><h3>Visual Odometry</h3></a>
              <p>
                Estimated trajectory of an autonomous vehicle using RANSAC for feature-matching and epipolar geometry and achieved an accuracy of 90%.
              </p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/visual_fusion.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/visual-sensor-fusion" target="_blank"><h3>Visual Sensor Fusion</h3></a>
              <p>
                Fused the output of YOLO detections and LIDAR point-cloud projections to detect humans, bicycles, and
                cars and their corresponding distance from the autonomous vehicle.
              </p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/disparity_heatmap.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/stereo-vision" target="_blank"><h3>Stereo Vision</h3></a>
              <p>
                Estimated depth and disparity from image sequences in the Middlebury dataset using epipolar geometry, rectification, 
                feature-correspondence, camera calibration, and template matching using sum of squared differences.
              </p>
            </td>
          </tr> -->

          

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/virtual_cube.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/AR-Tag-Detection" target="_blank"><h3>April tag detection and tracking</h3></a>
              <p>
                Detected and tracked an April tag in a video sequence by background removal using Fast Fourier Transform and Harris corner detection. 
              </p>
              <p>Decoded the detected April tag by identifying its position and orientation using homography.
              </p>
              <p>
                Superimposed a custom image and placed a 3D virtual cube on the detected April tag using homography, calibration, and projection matrices.
              </p>
            </td>
          </tr>
         
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/curved_lane_detection.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/lane-detection" target="_blank"><h3>Lane detection and turn prediction</h3></a>
              <p>
                Detected straight and curved road lanes in a video sequence mimicking the lane departure warning system in autonomous vehicles using hough transform for line detection, homography to get
                bird‚Äêeye‚Äôs view, polynomial curve fitting using sliding window technique. 
              </p>
              <p>
                Predicted the turn by calculating the radius of curvature of the detected lane.
              </p>
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/proj_mon_human_det.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/Monocular-Human-Position-Estimator" target="_blank"><h3>Human Position Estimator</h3></a>
              <p>
                Developed a software in C++ by Agile Iterative Process and test-driven development that detects and tracks humans using a pre-trained HOG 
                descriptor and SVM detector of OpenCV. 
              <p>
                Designed unit tests using Google Test, maintained version control using Git, checked build using Travis CI, and code coverage using Coverall.
              </p>
            </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/optical_flow.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/optical-flow" target="_blank"><h3>Optical Flow</h3></a>
              <p>
                Detected and tracked a moving vehicle and determined its speed using difference of images, contour detection, SIFT feature matching, pixel displacement, 
                and achieved an accuracy of 99% for different speeds. 
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <br>
        <br>
        <br> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Machine Learning Projects</heading>           
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody> -->
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/human-detection-tracking.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/lane-detection" target="_blank"><h3>Multiple human detection and tracking</h3></a>
              <p>
                Resulted in a Multi-Object Tracking Accuracy of 0.691 on multi‚Äêhuman tracking task by implementing a one‚Äêshot CNN tracker with two independent branches for 
                detection and re‚Äêidentification tasks. Jointly optimized the network for two tasks.
              </p>
              <!-- <p>
                Conceptualized a one‚Äêshot CNN tracker with two independent branches for detection and re‚Äêidentification tasks to perform multi‚Äêhuman tracking by training it on 
                CrowdHuman dataset and jointly optimizing the two tasks.
              </p>
              <p>
                Created a detection branch with three heads for heatmap, box offset, and bounding box dimension estimation, and a re‚Äêidentification branch to generate features to 
                distinguish objects. Achieved a multiple object tracking accuracy of 69.1
              </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/image-segmentation.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/image-segmentation" target="_blank"><h3>Image Segmentation</h3></a>
              <p>
                Achieved 99% accuracy and 0.97 dice score for semantic segmentation task by training U-Net network on Carvana dataset. 
                <!-- Implemented a CNN U-Net architecture to perform human segmentation by training the model on images in the MIT ADE20K dataset. -->
              </p>
              <!-- <p> -->
                <!-- Employed forward hooks from a pre-trained ResNet-152 encoder to relay data to the custom decoder. Achieved an Intersection of Union (IoU) score of 0.73. -->
              <!-- </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/image-denoising.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/image-denoising" target="_blank"><h3>Image Denoising</h3></a>
              <p>
                Improved the performance by 10% over baseline for image denoising task by formulating a convolutional autoencoder with skip connections and training it on 
                SID dataset.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/monkey-species-classification.jpg" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/transfer-learning" target="_blank"><h3>Image Classification</h3></a>
              <p>
                Increased the accuracy by 40% on monkey species classification task by leveraging transfer learning to fine‚Äêtune the pre‚Äê
                trained VGG‚Äê16 model. Trained the model on small dataset having 10 classes and 140 images per class.
              </p>         
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/steering-control.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/steering_control_using_cnn" target="_blank"><h3>CNN Steering Control</h3></a>
              <p>
                Designed a CNN to learn steering angle from the output of the camera attached to the autonomous vehicle. Generated a
                custom dataset by manually driving the car in simulator. Obtained an accuracy of 86%
                <!-- Designed a CNN to learn steering angle from the output of the camera attached to the autonomous vehicle.        -->
              </p>
              <!-- <p>
                Created a custom dataset by manually running the car on laps using Udacity‚Äôs self-driving car simulator. Resulted in a testing accuracy of 86%.
              </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/handwritten-digit.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/hand-written-digit-recognition" target="_blank"><h3>Hand-written digit recognition</h3></a>
              <p>
                Implemented Linear SVM, Kernel SVM with linear, polynomial, and RBF kernels, Logistic Regression, and LeNet-5 CNN architecture on the MNIST dataset to 
                recognize hand-written digits using any in-built functions and compared their results.
              </p>           
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/face-recognition.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/face-recognition" target="_blank"><h3>Face Recognition</h3></a>
              <p>
                Implemented Bayes' classifier, k-NN, Kernel and Boosted SVM along with dimensionality reduction using PCA and MDA for face recognition. Achieved an accuracy of 
                over 70% for each of them.
              </p>
            </td>
          </tr>
        </tbody></table>
        <br>
        <br>
        <br>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Robotics Software Projects</heading>           
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/home_organizing_robot.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/decluttering_domestic_robot" target="_blank"><h3>Home Organizing Robot</h3></a>
              <p>
                Developed a ROS package in C++ in by Agile Iterative Process with GitHub Continuous Integration and test-driven
                development using Google Test for Tiago mobile manipulator for indoor search and object manipulation.
              </p>
              <p>
                Used MoveBase for autonomous navigation, MoveIt for manipulator control, and OpenCV for filtering and object detection.
              </p>
              <p>
                Maintained software version control using Git, checked build using Travis CI, and attained a code coverage of 80%.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/proj_mon_human_det.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/Monocular-Human-Position-Estimator" target="_blank"><h3>Human Position Estimator</h3></a>
              <p>
                Created a software in C++ by Agile Iterative Process with object-oriented programming that detects and tracks humans using the pre‚Äêtrained HOG descriptor and SVM 
                detector of OpenCV.
              </p>
              <p>
                Designed unit tests using Google Test, maintained software version control using Git, checked build using Travis CI, and attained a code coverage of 91%.
              </p>
            </td>
          </tr>

        </tbody></table>

        <br>
        <br>
        <br>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Robotics Hardware Project</heading>           
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/drone.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <h3>Hands-on Aerial Robotics</h3>
              <p>
                Developed a C++ pipeline for real‚Äêtime detection and tracking of humans using monocular camera and YOLO‚Äêv5 classifier. 
              </p>
              <p>Used 1D LiDAR sensor for depth sensing. Programmed PD controllers for precise yaw and depth control of the drone.
              </p>
                Constructed a ROS node for off‚Äêboard position control of drone to autonomously follow a moving April tag, move through
                set way‚Äêpoints, and to move in a figure‚Äêof‚Äêeight.
              </p>
            </td>
          </tr>

          
          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/A_star.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/A-star_on_turtlebot" target="_blank"><h3>A* Algorithm</h3></a>
              <p>
                Implemented A* algorithm on a point robot and ROS Turtlebot considering its non-holonomic constraints.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/dijkstra.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/dijkstra" target="_blank"><h3>Dijkstra's Algorithm</h3></a>
              <p>
                Implemented Dijkstra's algorithm on a point robot to find the obstacle-free optimal path.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/8_puzzle_solver.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/8_puzzle_solver" target="_blank"><h3>8-puzzle solver using BFS</h3></a>
              <p>
                Solved a 8-puzzle problem using Breadth First Search.
              </p>
            </td>
          </tr> -->
        </tbody>

        <br>
        <br>
        <br>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Path Planning Project</heading>           
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/turtlebot.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/robot-path-planning" target="_blank"><h3>Robot Path Planning</h3></a>
              <p>
                Implemented BFS, Dijkstra, A*, and Real Time-RRT* algorithms on differential-drive robot ROS TurtleBot.
              </p>
            </td>
          </tr>

          <!-- <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/A_star.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/A-star_on_turtlebot" target="_blank"><h3>A* Algorithm</h3></a>
              <p>
                Implemented A* algorithm on a point robot and ROS Turtlebot considering its non-holonomic constraints.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/dijkstra.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/dijkstra" target="_blank"><h3>Dijkstra's Algorithm</h3></a>
              <p>
                Implemented Dijkstra's algorithm on a point robot to find the obstacle-free optimal path.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/8_puzzle_solver.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/8_puzzle_solver" target="_blank"><h3>8-puzzle solver using BFS</h3></a>
              <p>
                Solved a 8-puzzle problem using Breadth First Search.
              </p>
            </td>
          </tr> -->
        </tbody>

        <br>
        <br>
        <br>

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Sensor Fusion Projects</heading>           
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/esekf.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/error-state-extended-kalman-filter" target="_blank"><h3>Sensor Fusion using ES-EKF</h3></a>
              <p>
                Fused sensor data from IMU, LIDAR, GNSS using Error State Extended Kalman Filter to estimate pose of an autonomous vehicle.
            </td>
          </tr>
        </tbody></table>

        <br>
        <br>
        <br> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Control System Projects</heading>           
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/lateral-control.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/error-state-extended-kalman-filter" target="_blank"><h3>Lateral and Longitudinal Controller</h3></a>
              <p>
                Designed a PID controller for lateral and longitudinal control of autonomous vehicle considering vehicle dynamics in Python.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:2.5%;width:25%;vertical-align:middle;min-width:120px">
              <img src="images/Luenberger_observer_slow_motion.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />
            </td>
            <td style="padding:2.5%;width:75%;vertical-align:middle">
              <a href="https://github.com/abhijitmahalle/self-driving-vehicle-control" target="_blank"><h3>Gantry Crane Control</h3></a>
              <p>
                Designed a LQR and LQG controller for gantry crane, analyzed the controllability and observability, and implemented Kalman filter to account for 
                Gaussian noise in sensor measurements.
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info" target="_blank">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
